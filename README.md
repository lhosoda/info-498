# INFO 498 Project: Examining Biases Within LLM Generated Data
## Logan Hosoda, Jada Nguyen, Shirley Yao

### Overview
The extensive adoption of AI models, especially in hiring processes, has streamlined candidate evaluation. Nonetheless, there are rising concerns about these algorithms unintentionally reinforcing biases, which can result in unfair treatment and systemic inequalities. Our research focuses on this pressing issue, examining bias and demographic-related stereotypes in hiring data generated by language models like ChatGPT or Llama. 

### Research Questions:
- How does the diversity of training data used in AI-driven hiring tools impact their likelihood of discriminating against certain groups?
- Which aspects of AI-driven hiring tool design are most prone to introducing bias, and how does this affect job candidates from different backgrounds?
- What strategies can be used to reduce or eliminate bias in AI-driven hiring tools?

### Project Objectives: 
- Examining implicit biases in the context of LLM generated mock hiring data
- Observing the differences in algorithmic biases between OpenAI's ChatGPT 3.5 and Meta's Llama 2
- Create a algorithm that ranks applicants based on a given job description
- Use a logistic regression machine learning model to assist with visual representations about the biased data

### In This Repository:
- Overview documents (ex. team agreement, midterm report, etc.)
- Iterations of our ranking algorithms, logistic regression models, and dataset generation

### Instructions:
#### In final.ipynb:
- Change the "data" value to the desired dataset (either ChatGPT or Llama)
- Enter a job description when prompted
- Run the visualization code chunk to observe confusion matrix and ROC curve
* Note: previous iterations of the ipynb and data csv files can be found in the respective data and technical component directories

### Further Reading:
Medium Article: https://medium.com/@bndn13/bias-in-ai-driven-hiring-tools-a-comparative-study-of-chatgpt-and-llama-2-ef2cd8bc7384 

